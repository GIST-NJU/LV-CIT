# A Combinatorial Interaction Testing Method for Multi-Label Image Classifier

This repository contains the source code of LV-CIT, a black-box testing method that applies Combinatorial Interaction Testing (CIT) to systematically test the ability of classifiers to handle such correlations. It also contains the replication package to reproduce the results reported in the ISSRE 2024 paper.

#### File Structure

```
.
├─data        # Data and results directory
│  ├─combine  # images and results of LV-CIT
|  |  ├─0source_img/testset  # source images from voc/coco, split into different dictionaries by labels (omitted)
|  |  |  ├─VOC
|  |  |  |  ├─aeroplane  # images
|  |  |  |  └─...
|  |  |  └─COCO
|  |  |     ├─airplane  # images
|  |  |     └─...
|  |  ├─1covering_array  # covering arrays generated by three algorithms (LV-CIT, Baseline, and ACTS)
|  |  |  ├─adaptive random  # covering arrays generated by LV-CIT
|  |  |  ├─baseline  # covering arrays generated by Baseline
|  |  |  └─acts  # covering arrays generated by ACTS
|  |  ├─2matting_img/testset  # matting images by [YOLACT++](https://github.com/dbolya/yolact) from source images
|  |  |  ├─VOC_output  # YOLACT++ output of VOC images (omitted)
|  |  |  ├─VOC_output_model_any_pass  # results of first validation by DNN models (omitted)
|  |  |  ├─VOC_library  # object libraries of VOC images (results of second valudation by human)
|  |  |  ├─COCO_output  # YOLACT++ output of COCO images
|  |  |  ├─COCO_output_model_any_pass  # results of first validation by DNN models (omitted)
|  |  |  └─COCO_library  # object libraries of COCO images (results of second valudation by human) (omitted)
|  |  ├─3combine_img  # composite images by LV-CIT (omitted)
|  |  |  ├─VOC_20_v6_random_255_255_255_255_s1a0  # composite images of VOC by LV-CIT
|  |  |  └─COCO_80_v6_random_255_255_255_255_s1a0  # composite images of COCO by LV-CIT
|  |  ├─4results  # model outputs
|  |  |  ├─VOC_20_v6_random_255_255_255_255_s1a0  # results of different models on VOC
|  |  |  ├─COCO_80_v6_random_255_255_255_255_s1a0  # results of different models on COCO
|  |  |  └─google  # results of ATOM provided by its authors
|  |  └─5res_analyse  # results of RQ1-RQ3
|  ├─coco # COCO2014 dataset (omitted)
|  └─voc # VOC2007 dataset (omitted)
├─checkpoints (omitted)  # checkpoints released by authors of DNN models (see DNN Models Under Test)
|  └─org
|     ├─msrn
|     ├─asl
|     └─mlgcn
├─pretrained (omitted)  # pretrained model of MSRN (see DNN Models Under Test)
|  └─resnet101_for_msrn.pth.tar
├─models  # for DNN models under test
├─dataloaders  # dataloaders for different datasets
├─result_analyse  # for analysing results
├─ca_generator.py  # for generating covering arrays by LV-CIT and Baseline
├─check_libraries.py  # for first validation step by DNN models (to build the object libraries)
├─combiner.py  # for generating composite images
├─combine_main.py  # for executing the tests for composite images
├─combine_runner.py
├─default_main.py  # for executing the tests for VOC/COCO test(validation) sets to get outputs of Random method
├─default_runner.py
├─compare_random_select.py  # for selecting the random outputs from the outputs of Random method
├─util.py
├─requirements.txt
└─README.md
```

## Experiment Setup

LV-CIT is implemented in Python (version XX). The code is developed and tested under a Linux platform. 

1. Run the following command to install dependency packages (in the root directory of this repository):

```
pip install -r requirements.txt
```

2. Download the checkpoints of the DNN models under test, and put them in the `checkpoints` or `pretrained` directory.

| DNN Models | Dataset |
| :---: | :---: |
| [MSRN](https://github.com/chehao2628/MSRN) | VOC, COCO |
| [ML-GCN](https://github.com/megvii-research/ML-GCN) | VOC, COCO |
| [ASL](https://github.com/Alibaba-MIIL/ASL) | VOC, COCO |

3. Download the [VOC2007](http://host.robots.ox.ac.uk/pascal/VOC/voc2007/) and [COCO2014](https://cocodataset.org) datasets and put them in the `data` directory.

4. Download our created [object libraries](https://drive.google.com/drive/folders/1_z7JdSVLbSxoTLke3J6249HhtKztXlhj) and put them in the `data/combine/2matting_img/testset` directory.

## How to Run

#### 1. Generate Label Value Covering Arrays

Run the following code to generate label value covering arrays. The generation results can be found in `......` 

```bash
python ca_generator.py
```

#### 2. Build Object Libraries

> This step can be skipped if you downloaded our released object libraries

First, run the following code to generate matting images (see [YOLACT++](https://github.com/dbolya/yolact) for more details), and save these images in `data/combine/2matting_img/testset/<dataset>_output`, where `<dataset>` is `VOC` or `COCO`.

```bash
...
```

Next, sample by DNN models and human:

```bash
python check_libraries.py
```

The final object libraries are saved in `data/combine/2matting_img/testset/<dataset>_library`, where `<dataset>` is `VOC` or `COCO`.

#### 3. Generate Composite Images

Execute the following command to generate composite images:

```bash
python combiner.py
```

The execution of the above script relies on the label value covering arrays generated in Step 1 and the object libraries generated in Step 2. The results will be saved in `data/combine/3combine_img`.

#### 4. Execute Tests

Run the following code to use compsite images generated by LV-CIT to test the DNN models:

```bash
python combine_main.py
```

The results will be saved in `....`.

In addition to LV-CIT, run the following code to use randomly selected images (from test/validation sets of VOC and COCO) to test the DNN models (i.e., the Random method):

```bash
python default_main.py
python compare_random_select.py
```

#### 5. Result Analysis

Run the following commands to get the figures and tables presented in the paper:

```bash
python result_analyse/plot.py
python result_analyse/ana_atom_info.py
python result_analyse/ana_train_val_info.py
python result_analyse/analyse.py
```
